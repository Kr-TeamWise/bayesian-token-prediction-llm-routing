import pandas as pd
import numpy as np
from datasets import load_dataset
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class LMArenaDataLoader:
    """LMArena data loader and preprocessing class"""
    
    def __init__(self, cache_dir: str = "./data"):
        self.cache_dir = cache_dir
        self.raw_data = None
        self.processed_data = None
        
    def download_lmarena_data(self, dataset_name: str = "lmarena-ai/arena-human-preference-55k") -> pd.DataFrame:
        """
        Download LMArena data from Hugging Face
        
        Args:
            dataset_name: Name of dataset to download
            
        Returns:
            pd.DataFrame: Raw conversation data
        """
        print(f"ğŸ”„ Downloading LMArena data: {dataset_name}")
        
        try:
            # Load dataset from Hugging Face
            dataset = load_dataset(dataset_name, cache_dir=self.cache_dir)
            
            # Convert train split to DataFrame
            if 'train' in dataset:
                self.raw_data = dataset['train'].to_pandas()
            else:
                # Use first available split if train split doesn't exist
                first_split = list(dataset.keys())[0]
                self.raw_data = dataset[first_split].to_pandas()
                
            print(f"âœ… Data loading complete: {len(self.raw_data):,} conversations")
            print(f"ğŸ“Š Columns: {list(self.raw_data.columns)}")
            
            # Convert to match actual data structure
            processed_data = self._process_arena_data(self.raw_data)
            # Assign processed data to self.raw_data for use in preprocess_data
            self.raw_data = processed_data
            return processed_data
            
        except Exception as e:
            print(f"âŒ Data download failed: {e}")
            print("ğŸ’¡ Generating simulation data as alternative.")
            return self._generate_simulated_data()
    
    def _process_arena_data(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        """Convert actual LMArena data to experimental format"""
        print("ğŸ”„ Converting LMArena data format...")
        
        try:
            processed_conversations = []
            
            for idx, row in raw_data.iterrows():
                # Extract basic information
                conversation_data = {
                    'conversation_id': row['id'],
                    'model_a': row['model_a'],
                    'model_b': row['model_b'], 
                    'prompt': row['prompt'] if pd.notna(row['prompt']) else '',
                    'response_a': row['response_a'] if pd.notna(row['response_a']) else '',
                    'response_b': row['response_b'] if pd.notna(row['response_b']) else '',
                }
                
                # Process winner information (adapted to actual data structure)
                if row['winner_model_a'] == 1:
                    conversation_data['winner'] = 'model_a'
                elif row['winner_model_b'] == 1:
                    conversation_data['winner'] = 'model_b'
                elif row['winner_tie'] == 1:
                    conversation_data['winner'] = 'tie'
                else:
                    conversation_data['winner'] = 'unknown'
                
                # Time information (randomly generated within 9-month range)
                start_date = pd.Timestamp('2023-04-01')
                end_date = pd.Timestamp('2024-01-01')
                days_offset = int(np.random.exponential(120))
                days_offset = min(days_offset, 270)
                conversation_data['timestamp'] = start_date + pd.Timedelta(days=days_offset)
                
                # Extract query characteristics
                prompt_text = str(conversation_data['prompt'])
                
                # Handle case where prompt is in list format string
                if prompt_text.startswith('[') and prompt_text.endswith(']'):
                    try:
                        import ast
                        prompt_list = ast.literal_eval(prompt_text)
                        if isinstance(prompt_list, list):
                            prompt_text = ' '.join(str(p) for p in prompt_list)
                    except:
                        pass  # Use original if parsing fails
                
                conversation_data['query_length'] = len(prompt_text)
                conversation_data['query_complexity'] = self._estimate_complexity(prompt_text)
                conversation_data['query_type'] = self._classify_query_type(prompt_text)
                
                # Estimate response token count (more accurate method)
                response_a_text = str(conversation_data['response_a'])
                response_b_text = str(conversation_data['response_b'])
                
                # Token estimation based on word count (English: 1.3x, other languages: 1.5x)
                def estimate_tokens(text):
                    if not text or text == 'nan':
                        return 50  # Default value
                    words = len(text.split())
                    # Approximate token count (based on GPT tokenizer)
                    return max(10, int(words * 1.3))
                
                conversation_data['response_tokens_a'] = estimate_tokens(response_a_text)
                conversation_data['response_tokens_b'] = estimate_tokens(response_b_text)
                
                processed_conversations.append(conversation_data)
                
                # Progress output (every 5000 entries)
                if idx % 5000 == 0:
                    print(f"  ğŸ“ˆ Processing... {idx:,}/{len(raw_data):,}")
            
            result_df = pd.DataFrame(processed_conversations)
            print(f"âœ… Conversion complete: {len(result_df):,} conversations")
            
            # Data quality check
            print(f"ğŸ“Š Data quality check:")
            print(f"  â€¢ Unique models A: {result_df['model_a'].nunique()}")
            print(f"  â€¢ Unique models B: {result_df['model_b'].nunique()}")
            print(f"  â€¢ Winner distribution: {result_df['winner'].value_counts().to_dict()}")
            print(f"  â€¢ Query type distribution: {result_df['query_type'].value_counts().to_dict()}")
            print(f"  â€¢ Average query length: {result_df['query_length'].mean():.1f}")
            print(f"  â€¢ Average response tokens A: {result_df['response_tokens_a'].mean():.1f}")
            print(f"  â€¢ Average response tokens B: {result_df['response_tokens_b'].mean():.1f}")
            
            return result_df
            
        except Exception as e:
            print(f"âŒ Data conversion failed: {e}")
            print("ğŸ’¡ Replacing with simulation data.")
            return self._generate_simulated_data()
    
    def _estimate_complexity(self, text: str) -> float:
        """Estimate text complexity (improved version)"""
        if not text or text == 'nan':
            return 0.5
        
        text = str(text).lower()
        complexity_score = 0.2  # Base score
        
        # Length-based complexity
        if len(text) > 500:
            complexity_score += 0.3
        elif len(text) > 200:
            complexity_score += 0.2
        elif len(text) > 100:
            complexity_score += 0.1
        
        # Sentence count-based
        sentences = text.count('.') + text.count('!') + text.count('?')
        if sentences > 5:
            complexity_score += 0.15
        elif sentences > 2:
            complexity_score += 0.1
        
        # Complex keywords-based
        complex_keywords = [
            'algorithm', 'analysis', 'technical', 'code', 'math', 'explain', 'complex',
            'implementation', 'optimization', 'architecture', 'design', 'strategy',
            'reasoning', 'logic', 'computation', 'methodology', 'framework'
        ]
        keyword_count = sum(1 for keyword in complex_keywords if keyword in text)
        complexity_score += min(0.3, keyword_count * 0.05)
        
        # Detect code blocks or equations
        if '```' in text or '$$' in text or 'def ' in text or 'class ' in text:
            complexity_score += 0.2
        
        return min(1.0, complexity_score)
    
    def _classify_query_type(self, text: str) -> str:
        """Classify query type (improved version)"""
        if not text or text == 'nan':
            return 'general'
        
        text = str(text).lower()
        
        # Coding-related
        coding_keywords = ['code', 'programming', 'function', 'python', 'javascript', 
                          'algorithm', 'debug', 'syntax', 'implementation', '```']
        if any(word in text for word in coding_keywords):
            return 'coding'
        
        # Math-related  
        math_keywords = ['math', 'calculate', 'equation', 'solve', 'formula', 
                        'theorem', 'proof', 'derivative', 'integral']
        if any(word in text for word in math_keywords):
            return 'math'
        
        # Creative-related
        creative_keywords = ['creative', 'story', 'poem', 'write', 'imagine', 
                           'fiction', 'narrative', 'character', 'plot']
        if any(word in text for word in creative_keywords):
            return 'creative'
        
        # Analysis-related
        analysis_keywords = ['analyze', 'comparison', 'evaluate', 'assess', 
                           'review', 'critique', 'examine', 'investigate']
        if any(word in text for word in analysis_keywords):
            return 'analysis'
        
        return 'knowledge'  # Default
    
    def _generate_simulated_data(self, n_samples: int = 50000) -> pd.DataFrame:
        """
        ì‹¤ì œ LMArena íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
        (ì‹¤ì œ ë°ì´í„°ì— ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ì‚¬ìš©)
        """
        print(f"ğŸ² ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì¤‘: {n_samples:,} ìƒ˜í”Œ")
        
        # ì‹¤ì œ LMArenaì—ì„œ ê´€ì°°ëœ ëª¨ë¸ë“¤
        models = [
            'gpt-4-0613', 'gpt-4-1106-preview', 'gpt-3.5-turbo-0613',
            'claude-2.1', 'claude-2.0', 'claude-instant-1',
            'gemini-pro', 'llama-2-70b-chat', 'llama-2-13b-chat',
            'mixtral-8x7b-instruct'
        ]
        
        # ëª¨ë¸ í’ˆì§ˆ ì ìˆ˜ (Elo ê¸°ë°˜)
        model_quality = {
            'gpt-4-1106-preview': 1250,
            'gpt-4-0613': 1200,
            'claude-2.1': 1180,
            'claude-2.0': 1150,
            'gemini-pro': 1140,
            'mixtral-8x7b-instruct': 1120,
            'llama-2-70b-chat': 1080,
            'gpt-3.5-turbo-0613': 1050,
            'llama-2-13b-chat': 1000,
            'claude-instant-1': 970
        }
        
        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
        np.random.seed(42)
        data = []
        
        # ì‹œê°„ ë²”ìœ„ (9ê°œì›”)
        start_date = pd.Timestamp('2023-04-01')
        end_date = pd.Timestamp('2024-01-01')
        
        for i in range(n_samples):
            # ì‹œê°„ ìƒì„± (ì§€ìˆ˜ ë¶„í¬ë¡œ ìµœê·¼ ë°ì´í„° ë” ë§ì´)
            days_offset = int(np.random.exponential(120))
            days_offset = min(days_offset, 270)
            timestamp = start_date + pd.Timedelta(days=days_offset)
            
            # ëª¨ë¸ ìŒ ì„ íƒ
            model_a, model_b = np.random.choice(models, 2, replace=False)
            
            # ì¿¼ë¦¬ ìœ í˜•
            query_types = ['coding', 'math', 'creative', 'analysis', 'knowledge']
            query_type = np.random.choice(query_types)
            
            # ì¿¼ë¦¬ ê¸¸ì´ì™€ ë³µì¡ë„
            if query_type == 'coding':
                query_length = int(np.random.normal(80, 20))
                complexity = np.random.uniform(0.6, 0.9)
            elif query_type == 'math':
                query_length = int(np.random.normal(45, 15))
                complexity = np.random.uniform(0.7, 1.0)
            elif query_type == 'creative':
                query_length = int(np.random.normal(60, 25))
                complexity = np.random.uniform(0.3, 0.7)
            elif query_type == 'analysis':
                query_length = int(np.random.normal(90, 30))
                complexity = np.random.uniform(0.5, 0.8)
            else:  # knowledge
                query_length = int(np.random.normal(35, 12))
                complexity = np.random.uniform(0.2, 0.6)
            
            query_length = max(10, query_length)
            
            # ìŠ¹ì ê²°ì • (í’ˆì§ˆ ì ìˆ˜ + ë³µì¡ë„ ë³´ì • + ë…¸ì´ì¦ˆ)
            quality_a = model_quality[model_a]
            quality_b = model_quality[model_b]
            
            # ë³µì¡í•œ ì¿¼ë¦¬ì¼ìˆ˜ë¡ ê³ ê¸‰ ëª¨ë¸ì—ê²Œ ìœ ë¦¬
            complex_bonus_a = complexity * 50 if quality_a > 1100 else 0
            complex_bonus_b = complexity * 50 if quality_b > 1100 else 0
            
            # ìµœì¢… ì ìˆ˜
            score_a = quality_a + complex_bonus_a + np.random.normal(0, 50)
            score_b = quality_b + complex_bonus_b + np.random.normal(0, 50)
            
            if abs(score_a - score_b) < 30:
                winner = 'tie'
            elif score_a > score_b:
                winner = 'model_a'
            else:
                winner = 'model_b'
            
            # ì‘ë‹µ í† í° ìˆ˜ ì‹œë®¬ë ˆì´ì…˜ - ë” í˜„ì‹¤ì ìœ¼ë¡œ
            # ëª¨ë¸ë³„ í† í° ìƒì„± íŠ¹ì„±
            model_token_patterns = {
                'gpt-4-1106-preview': {'base': 180, 'std': 65, 'complexity_factor': 1.3},
                'gpt-4-0613': {'base': 175, 'std': 60, 'complexity_factor': 1.2},
                'claude-2.1': {'base': 220, 'std': 80, 'complexity_factor': 1.5},
                'claude-2.0': {'base': 210, 'std': 75, 'complexity_factor': 1.4},
                'claude-instant-1': {'base': 120, 'std': 45, 'complexity_factor': 0.8},
                'gemini-pro': {'base': 160, 'std': 55, 'complexity_factor': 1.1},
                'llama-2-70b-chat': {'base': 140, 'std': 50, 'complexity_factor': 1.0},
                'llama-2-13b-chat': {'base': 100, 'std': 40, 'complexity_factor': 0.7},
                'mixtral-8x7b-instruct': {'base': 155, 'std': 52, 'complexity_factor': 1.0},
                'gpt-3.5-turbo-0613': {'base': 135, 'std': 48, 'complexity_factor': 0.9}
            }
            
            # ì¿¼ë¦¬ ìœ í˜•ë³„ í† í° ìŠ¹ìˆ˜
            type_multipliers = {
                'coding': 1.6,  # ì½”ë”©ì€ ë” ê¸´ ì‘ë‹µ
                'math': 1.2,    # ìˆ˜í•™ì€ ì¤‘ê°„ ê¸¸ì´
                'creative': 1.8, # ì°½ì‘ì€ ê°€ì¥ ê¸´ ì‘ë‹µ
                'analysis': 1.4, # ë¶„ì„ì€ ìƒë‹¹íˆ ê¸´ ì‘ë‹µ
                'knowledge': 0.8 # ì§€ì‹ ì§ˆë¬¸ì€ ì§§ì€ ì‘ë‹µ
            }
            
            # ëª¨ë¸ A í† í° ìƒì„±
            pattern_a = model_token_patterns[model_a]
            base_tokens_a = pattern_a['base'] + np.random.normal(0, pattern_a['std'])
            complexity_bonus_a = complexity * pattern_a['complexity_factor'] * 30
            type_bonus_a = base_tokens_a * (type_multipliers[query_type] - 1)
            length_factor_a = min(2.0, np.log1p(query_length) / 3.5)  # ì¿¼ë¦¬ ê¸¸ì´ ì˜í–¥
            
            # ëœë¤ ë…¸ì´ì¦ˆì™€ ì´ìƒì¹˜
            if np.random.random() < 0.05:  # 5% í™•ë¥ ë¡œ ì´ìƒì¹˜
                noise_a = np.random.uniform(-50, 150)
            else:
                noise_a = np.random.normal(0, 20)
            
            response_tokens_a = max(15, int(base_tokens_a + complexity_bonus_a + type_bonus_a * length_factor_a + noise_a))
            
            # ëª¨ë¸ B í† í° ìƒì„± (ë…ë¦½ì )
            pattern_b = model_token_patterns[model_b]
            base_tokens_b = pattern_b['base'] + np.random.normal(0, pattern_b['std'])
            complexity_bonus_b = complexity * pattern_b['complexity_factor'] * 30
            type_bonus_b = base_tokens_b * (type_multipliers[query_type] - 1)
            length_factor_b = min(2.0, np.log1p(query_length) / 3.5)
            
            if np.random.random() < 0.05:  # 5% í™•ë¥ ë¡œ ì´ìƒì¹˜
                noise_b = np.random.uniform(-50, 150)
            else:
                noise_b = np.random.normal(0, 20)
                
            response_tokens_b = max(15, int(base_tokens_b + complexity_bonus_b + type_bonus_b * length_factor_b + noise_b))
            
            data.append({
                'conversation_id': f"conv_{i:06d}",
                'timestamp': timestamp,
                'model_a': model_a,
                'model_b': model_b,
                'query_type': query_type,
                'query_length': query_length,
                'query_complexity': complexity,
                'response_tokens_a': response_tokens_a,
                'response_tokens_b': response_tokens_b,
                'winner': winner,
                'quality_a': quality_a,
                'quality_b': quality_b
            })
        
        self.raw_data = pd.DataFrame(data)
        print(f"âœ… ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        return self.raw_data
    
    def preprocess_data(self) -> pd.DataFrame:
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ"""
        if self.raw_data is None:
            raise ValueError("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. download_lmarena_data()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        df = self.raw_data.copy()
        
        # ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ (ì´ë¯¸ ë³€í™˜ëœ ë°ì´í„°ì—ì„œ)
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df['hour'] = df['timestamp'].dt.hour
            df['day_of_week'] = df['timestamp'].dt.dayofweek
            df['month'] = df['timestamp'].dt.month
        else:
            # timestampê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ìƒì„±
            print("âš ï¸ timestamp ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì„ì˜ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            start_date = pd.Timestamp('2023-04-01')
            end_date = pd.Timestamp('2024-01-01')
            timestamps = []
            for i in range(len(df)):
                days_offset = int(np.random.exponential(120))
                days_offset = min(days_offset, 270)
                timestamps.append(start_date + pd.Timedelta(days=days_offset))
            
            df['timestamp'] = pd.to_datetime(timestamps)
            df['hour'] = df['timestamp'].dt.hour
            df['day_of_week'] = df['timestamp'].dt.dayofweek
            df['month'] = df['timestamp'].dt.month
        
        # ì¿¼ë¦¬ íŠ¹ì„± ì •ê·œí™” (ì´ë¯¸ ë³€í™˜ëœ ë°ì´í„°ì—ì„œ)
        if 'query_length' in df.columns:
            df['query_tokens'] = df['query_length'] / 4  # ëŒ€ëµì  í† í° ìˆ˜
            df['length_normalized'] = np.log1p(df['query_length'])
        else:
            # ê¸°ë³¸ê°’ ì„¤ì •
            df['query_length'] = 50
            df['query_tokens'] = 12.5
            df['length_normalized'] = np.log1p(50)
            df['query_complexity'] = 0.5
            df['query_type'] = 'knowledge'
        
        # ëª¨ë¸ ê°€ì¡± ë¶„ë¥˜
        def get_model_family(model_name):
            model_lower = model_name.lower()
            if 'gpt' in model_lower or 'chatgpt' in model_lower:
                return 'openai'
            elif 'claude' in model_lower:
                return 'anthropic'
            elif 'gemini' in model_lower or 'bard' in model_lower:
                return 'google'
            elif 'llama' in model_lower:
                return 'meta'
            elif 'mixtral' in model_lower or 'mistral' in model_lower:
                return 'mistral'
            elif 'vicuna' in model_lower:
                return 'lmsys'
            elif 'alpaca' in model_lower:
                return 'stanford'
            elif 'wizardlm' in model_lower:
                return 'microsoft'
            elif 'palm' in model_lower:
                return 'google'
            else:
                return 'other'
        
        df['family_a'] = df['model_a'].apply(get_model_family)
        df['family_b'] = df['model_b'].apply(get_model_family)
        
        # ë¹„ìš© ì •ë³´ ì¶”ê°€ (í† í°ë‹¹ ë¹„ìš© - ì‹¤ì œ ëª¨ë¸ë“¤ í¬í•¨)
        cost_per_1k_tokens = {
            # OpenAI models
            'gpt-4-1106-preview': 0.03,
            'gpt-4-0613': 0.03,
            'gpt-4': 0.03,
            'gpt-4-turbo': 0.03,
            'gpt-3.5-turbo-0613': 0.002,
            'gpt-3.5-turbo': 0.002,
            'chatgpt': 0.002,
            
            # Anthropic models
            'claude-2.1': 0.024,
            'claude-2.0': 0.024,
            'claude-2': 0.024,
            'claude-instant-1': 0.00163,
            'claude-instant': 0.00163,
            'claude-3-opus-20240229': 0.075,
            'claude-3-sonnet-20240229': 0.015,
            'claude-3-haiku-20240307': 0.00125,
            
            # Google models
            'gemini-pro': 0.0005,
            'palm-2': 0.001,
            'bard': 0.0005,
            
            # Meta models
            'llama-2-70b-chat': 0.00275,
            'llama-2-13b-chat': 0.0013,
            'llama-2-7b-chat': 0.001,
            
            # Mistral models
            'mixtral-8x7b-instruct': 0.00024,
            'mistral-7b-instruct': 0.0002,
            
            # Open source / research models (estimated costs)
            'vicuna-33b': 0.002,
            'vicuna-13b': 0.001,
            'vicuna-7b': 0.0008,
            'alpaca-7b': 0.0008,
            'wizardlm-70b': 0.003,
            'wizardlm-13b': 0.001,
        }
        
        # í† í° ìˆ˜ê°€ ì—†ìœ¼ë©´ ì¶”ì • (ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì—ì„œëŠ” ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
        if 'response_tokens_a' not in df.columns:
            print("âš ï¸ í† í° ìˆ˜ ì»¬ëŸ¼ì´ ì—†ì–´ì„œ ê¸°ë³¸ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤.")
            df['response_tokens_a'] = 150
            df['response_tokens_b'] = 150
        else:
            print(f"âœ… ê¸°ì¡´ í† í° ìˆ˜ ìœ ì§€: A í‰ê· ={df['response_tokens_a'].mean():.1f}, B í‰ê· ={df['response_tokens_b'].mean():.1f}")
            print(f"   í† í° ìˆ˜ ë¶„í¬: A std={df['response_tokens_a'].std():.1f}, B std={df['response_tokens_b'].std():.1f}")
            print(f"   í† í° ìˆ˜ ë²”ìœ„: A {df['response_tokens_a'].min()}-{df['response_tokens_a'].max()}, B {df['response_tokens_b'].min()}-{df['response_tokens_b'].max()}")
            print(f"   ê³ ìœ  í† í° ê°’: A {df['response_tokens_a'].nunique()}ê°œ, B {df['response_tokens_b'].nunique()}ê°œ")
            # ê¸°ì¡´ í† í° ìˆ˜ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€ (ë®ì–´ì“°ì§€ ì•ŠìŒ)
        
        def get_cost(model, tokens):
            return (tokens / 1000) * cost_per_1k_tokens.get(model, 0.01)
        
        df['cost_a'] = df.apply(lambda row: get_cost(row['model_a'], row['response_tokens_a']), axis=1)
        df['cost_b'] = df.apply(lambda row: get_cost(row['model_b'], row['response_tokens_b']), axis=1)
        
        # í’ˆì§ˆ ì •ê·œí™” (0-1 ìŠ¤ì¼€ì¼) - ì„ì˜ë¡œ ìƒì„±
        if 'quality_a' not in df.columns:
            # ëª¨ë¸ë³„ ëŒ€ëµì  í’ˆì§ˆ ì ìˆ˜ (Elo ê¸°ë°˜ ì¶”ì •)
            quality_map = {
                # Tier 1: Top performing models
                'gpt-4': 1250, 'gpt-4-turbo': 1250, 'gpt-4-1106-preview': 1250, 'gpt-4-0613': 1200,
                'claude-3-opus-20240229': 1240, 'claude-2.1': 1180, 'claude-2.0': 1150, 'claude-2': 1150,
                'claude-3-sonnet-20240229': 1170, 'claude-3-haiku-20240307': 1100,
                
                # Tier 2: Strong models
                'gemini-pro': 1140, 'mixtral-8x7b-instruct': 1120, 'llama-2-70b-chat': 1080,
                'gpt-3.5-turbo': 1050, 'gpt-3.5-turbo-0613': 1050, 'chatgpt': 1050,
                'claude-instant-1': 970, 'claude-instant': 970,
                
                # Tier 3: Mid-range models
                'vicuna-33b': 1020, 'llama-2-13b-chat': 1000, 'wizardlm-70b': 1010,
                'palm-2': 980, 'bard': 980,
                
                # Tier 4: Smaller models
                'vicuna-13b': 950, 'wizardlm-13b': 940, 'llama-2-7b-chat': 920,
                'vicuna-7b': 900, 'alpaca-7b': 880, 'mistral-7b-instruct': 930
            }
            
            # ê¸°ë³¸ê°’ ì„¤ì • (ë§¤í•‘ì— ì—†ëŠ” ëª¨ë¸ë“¤)
            def get_quality_score(model_name):
                if model_name in quality_map:
                    return quality_map[model_name]
                else:
                    # ëª¨ë¸ ì´ë¦„ì—ì„œ ìœ ì¶”
                    model_lower = model_name.lower()
                    if '70b' in model_lower or '65b' in model_lower:
                        return 1050  # í° ëª¨ë¸
                    elif '30b' in model_lower or '33b' in model_lower:
                        return 1000  # ì¤‘ê°„ ëª¨ë¸
                    elif '13b' in model_lower or '15b' in model_lower:
                        return 950   # ì‘ì€ ëª¨ë¸
                    elif '7b' in model_lower or '6b' in model_lower:
                        return 900   # ë§¤ìš° ì‘ì€ ëª¨ë¸
                    else:
                        return 1000  # ê¸°ë³¸ê°’
            
            df['quality_a'] = df['model_a'].apply(get_quality_score)
            df['quality_b'] = df['model_b'].apply(get_quality_score)
            
            # ì •ê·œí™”
            min_quality = 850
            max_quality = 1300
            df['quality_normalized_a'] = (df['quality_a'] - min_quality) / (max_quality - min_quality)
            df['quality_normalized_b'] = (df['quality_b'] - min_quality) / (max_quality - min_quality)
        
        self.processed_data = df
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ìµœì¢… ë°ì´í„°: {len(df):,} ê°œ ëŒ€í™”")
        return df
    
    def get_temporal_splits(self, train_ratio: float = 0.6, val_ratio: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ì‹œê³„ì—´ ê¸°ë°˜ ë°ì´í„° ë¶„í• """
        if self.processed_data is None:
            raise ValueError("ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocess_data()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ì‹œê°„ìˆœ ì •ë ¬
        df_sorted = self.processed_data.sort_values('timestamp').reset_index(drop=True)
        
        n_total = len(df_sorted)
        train_end = int(train_ratio * n_total)
        val_end = int((train_ratio + val_ratio) * n_total)
        
        # ë²„í¼ êµ¬ê°„ (ì¼ì£¼ì¼)
        buffer_samples = max(1, int(0.02 * n_total))
        
        train_data = df_sorted.iloc[:train_end]
        val_data = df_sorted.iloc[train_end + buffer_samples:val_end]
        test_data = df_sorted.iloc[val_end + buffer_samples:]
        
        print(f"ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        print(f"  ğŸ”¹ í›ˆë ¨: {len(train_data):,} ê°œ ({train_data['timestamp'].min()} ~ {train_data['timestamp'].max()})")
        print(f"  ğŸ”¹ ê²€ì¦: {len(val_data):,} ê°œ ({val_data['timestamp'].min()} ~ {val_data['timestamp'].max()})")
        print(f"  ğŸ”¹ í…ŒìŠ¤íŠ¸: {len(test_data):,} ê°œ ({test_data['timestamp'].min()} ~ {test_data['timestamp'].max()})")
        
        return train_data, val_data, test_data
    
    def get_summary_stats(self) -> Dict:
        """ë°ì´í„° ìš”ì•½ í†µê³„"""
        if self.processed_data is None:
            return {}
        
        df = self.processed_data
        
        stats = {
            'total_conversations': len(df),
            'unique_models': df[['model_a', 'model_b']].stack().nunique(),
            'date_range': (df['timestamp'].min(), df['timestamp'].max()),
            'most_common_models': df[['model_a', 'model_b']].stack().value_counts().head(10).to_dict()
        }
        
        # winner ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if 'winner' in df.columns:
            stats['winner_distribution'] = df['winner'].value_counts().to_dict()
        
        # query_type ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if 'query_type' in df.columns:
            stats['query_types'] = df['query_type'].value_counts().to_dict()
        
        # query_length ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if 'query_length' in df.columns:
            stats['avg_query_length'] = df['query_length'].mean()
        
        # query_complexity ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if 'query_complexity' in df.columns:
            stats['avg_complexity'] = df['query_complexity'].mean()
        
        return stats 