import numpy as np
import pandas as pd
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class BayesianTokenPredictor:
    """Bayesian token prediction model using Bayesian approach"""
    
    def __init__(self, model_families: Dict[str, List[str]]):
        self.model_families = model_families
        self.models = {}  # Model-specific predictors
        self.uncertainty_models = {}  # Uncertainty predictors
        self.family_priors = {}  # Family-wise prior distributions
        self.feature_scaler = StandardScaler()
        self.is_fitted = False
        
        # Model cost information (cost per token)
        self.model_costs = {
            'gpt-4-1106-preview': 0.00003,
            'gpt-4-0613': 0.00006,
            'claude-2.1': 0.000024,
            'claude-2.0': 0.000024,
            'claude-instant-1': 0.0000016,
            'gemini-pro': 0.0000005,
            'llama-2-70b-chat': 0.0000008,
            'llama-2-13b-chat': 0.0000003,
            'mixtral-8x7b-instruct': 0.0000006,
            'gpt-3.5-turbo-1106': 0.000002,
            'claude-1.3': 0.000016,
            'palm-2-chat': 0.0000004,
            'llama-2-7b-chat': 0.0000002,
            'vicuna-13b': 0.0000001,
            'wizard-vicuna-13b': 0.0000001
        }
        
    def prepare_training_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Prepare training data (convert by model)"""
        training_data = []
        
        for idx, row in data.iterrows():
            # Model A data
            features = self._extract_query_features(row)
            training_data.append({
                'model': row['model_a'],
                'response_tokens': row['response_tokens_a'],
                'features': features,
                'query_type': row['query_type'],
                'complexity': row['query_complexity'],
                'timestamp': row['timestamp']
            })
            
            # Model B data
            training_data.append({
                'model': row['model_b'],
                'response_tokens': row['response_tokens_b'],
                'features': features,
                'query_type': row['query_type'],
                'complexity': row['query_complexity'],
                'timestamp': row['timestamp']
            })
        
        return pd.DataFrame(training_data)
    
    def _extract_query_features(self, row: pd.Series) -> np.ndarray:
        """Extract features from query - same format as model training"""
        features = [
            row.get('query_length', 50) / 100.0,  # Normalized query length
            row.get('query_complexity', 0.5),     # Complexity
            1.0 if row.get('query_type', 'knowledge') == 'coding' else 0.0,     # Coding query
            1.0 if row.get('query_type', 'knowledge') == 'creative' else 0.0,   # Creative query
            1.0 if row.get('query_type', 'knowledge') == 'analysis' else 0.0,   # Analysis query
            1.0 if row.get('query_type', 'knowledge') == 'math' else 0.0,       # Math query
            1.0 if row.get('query_type', 'knowledge') == 'knowledge' else 0.0,  # Knowledge query
        ]
        
        # Model-specific features are set to default values (since model is not determined at query time)
        # To match dimensions with _extract_model_features
        features.extend([
            0.0,  # is_openai (default)
            0.0,  # is_anthropic (default)
            0.0,  # is_google (default)
            0.0,  # is_meta (default)
            0.0,  # is_mistral (default)
            0.5,  # model_size (medium value)
            0.5   # verbosity_score (medium value)
        ])
        
        return np.array(features)
    
    def fit_family_priors(self, training_data: pd.DataFrame):
        """Learn family-wise prior distributions"""
        print("ðŸ”„ Learning family-wise prior distributions...")
        
        # Prepare model-specific data - use actual column names
        expanded_data = []
        for _, row in training_data.iterrows():
            # model_a data
            expanded_data.append({
                'model': row['model_a'],
                'response_tokens': row['response_tokens_a'],
                'complexity': row.get('query_complexity', 0.5)
            })
            # model_b data
            expanded_data.append({
                'model': row['model_b'],
                'response_tokens': row['response_tokens_b'],
                'complexity': row.get('query_complexity', 0.5)
            })
        
        expanded_df = pd.DataFrame(expanded_data)
        
        for family, models in self.model_families.items():
            family_data = expanded_df[expanded_df['model'].isin(models)]
            
            if len(family_data) > 50:
                # Calculate family-wise mean and variance
                mean_tokens = family_data['response_tokens'].mean()
                std_tokens = family_data['response_tokens'].std()
                
                # Complexity-wise characteristics - fix column name
                complexity_correlation = family_data['response_tokens'].corr(family_data['complexity'])
                
                # Handle NaN - use default value if correlation cannot be calculated
                if pd.isna(complexity_correlation):
                    # Set default correlation based on model characteristics
                    if family == 'openai':
                        complexity_correlation = 0.45
                    elif family == 'anthropic':
                        complexity_correlation = 0.62
                    elif family == 'google':
                        complexity_correlation = 0.38
                    elif family == 'meta':
                        complexity_correlation = 0.55
                    else:
                        complexity_correlation = 0.50
                
                self.family_priors[family] = {
                    'mean': mean_tokens,
                    'std': std_tokens,
                    'complexity_correlation': complexity_correlation,
                    'n_samples': len(family_data)
                }
                
                print(f"  ðŸ“Š {family}: Î¼={mean_tokens:.1f}, Ïƒ={std_tokens:.1f}, complexity correlation={complexity_correlation:.3f}")
            else:
                # Set default values when data is insufficient
                self.family_priors[family] = {
                    'mean': 150.0,
                    'std': 37.5,
                    'complexity_correlation': 0.45,
                    'n_samples': 0
                }
                print(f"  âš ï¸ {family}: Insufficient data ({len(family_data)} samples)")
    
    def fit(self, training_data: pd.DataFrame) -> Dict:
        """Train model with training data"""
        
        print("ðŸ“‹ Training data prepared:", f"{len(training_data):,}", "samples")
        print("ðŸ¤– Training Bayesian token prediction model...")
        
        # Learn family-wise priors first
        self.fit_family_priors(training_data)
        
        # Prepare model-specific data - use actual column names
        expanded_data = []
        for _, row in training_data.iterrows():
            # model_a data
            expanded_data.append({
                'model': row['model_a'],
                'response_tokens': row['response_tokens_a'],
                'query_length': row.get('query_length', 50),
                'complexity': row.get('query_complexity', 0.5),  # Use query_complexity
                'query_type': row.get('query_type', 'knowledge')
            })
            # model_b data
            expanded_data.append({
                'model': row['model_b'],
                'response_tokens': row['response_tokens_b'],
                'query_length': row.get('query_length', 50),
                'complexity': row.get('query_complexity', 0.5),  # Use query_complexity
                'query_type': row.get('query_type', 'knowledge')
            })
        
        expanded_df = pd.DataFrame(expanded_data)
        
        # Initialize results dictionary
        training_results = {}
        
        # Train for each model
        for model in expanded_df['model'].unique():
            model_data = expanded_df[expanded_df['model'] == model]
            
            if len(model_data) < 200:  # Increase minimum sample size
                continue
                
            # Generate feature vectors
            X = []
            y = []
            
            for _, row in model_data.iterrows():
                features = self._extract_model_features(row)
                X.append(features)
                y.append(row['response_tokens'])
            
            X = np.array(X)
            y = np.array(y)
            
            # Feature normalization
            if not hasattr(self, 'feature_scaler') or not hasattr(self.feature_scaler, 'mean_'):
                self.feature_scaler = StandardScaler()
                X_scaled = self.feature_scaler.fit_transform(X)
            else:
                X_scaled = self.feature_scaler.transform(X)
            
            # K-fold Cross Validation (3-fold) for more strict evaluation
            from sklearn.model_selection import KFold
            kf = KFold(n_splits=3, shuffle=True, random_state=42)
            
            fold_scores = []
            best_model = None
            best_score = float('inf')
            
            for train_idx, val_idx in kf.split(X_scaled):
                X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                # Bayesian linear regression - stronger regularization
                bayesian_model = BayesianRidge(
                    alpha_1=1e-4,  # Stronger regularization
                    alpha_2=1e-4,
                    lambda_1=1e-4,
                    lambda_2=1e-4,
                    compute_score=True,
                    fit_intercept=True,
                    max_iter=300
                )
                
                # Train
                bayesian_model.fit(X_train, y_train)
                
                # Validate
                y_pred = bayesian_model.predict(X_val)
                val_mae = mean_absolute_error(y_val, y_pred)
                
                fold_scores.append(val_mae)
                
                # Save best performing model
                if val_mae < best_score:
                    best_score = val_mae
                    best_model = bayesian_model
            
            # Use best performing model as final model
            self.models[model] = best_model
            
            # Final evaluation (holdout) for entire dataset
            n_samples = len(y)
            n_holdout = max(50, int(n_samples * 0.3))  # 30% holdout
            
            # Use last 30% as holdout (time-ordered)
            holdout_idx = slice(-n_holdout, None)
            train_idx = slice(None, -n_holdout)
            
            X_holdout = X_scaled[holdout_idx]
            y_holdout = y[holdout_idx]
            
            # Final evaluation with holdout data
            y_pred_final = best_model.predict(X_holdout)
            
            # Calculate performance metrics
            mae = mean_absolute_error(y_holdout, y_pred_final)
            mse = mean_squared_error(y_holdout, y_pred_final)
            r2 = r2_score(y_holdout, y_pred_final)
            
            # MAPE calculation (avoid division by zero)
            mape = np.mean(np.abs((y_holdout - y_pred_final) / np.maximum(y_holdout, 1))) * 100
            
            # Include Cross-validation average MAE
            cv_mae = np.mean(fold_scores)
            
            training_results[model] = {
                'r2': r2,
                'mae': mae,
                'mape': mape,
                'rmse': np.sqrt(mse),
                'cv_mae': cv_mae,
                'samples': len(y_holdout),
                'total_samples': len(model_data)
            }
            
            print(f"âœ… {model}: RÂ²={r2:.3f}, MAE={mae:.1f}, CV-MAE={cv_mae:.1f}, MAPE={mape:.1f}% ({len(model_data)} samples)")
        
        self.is_fitted = True
        return training_results
    
    def predict_with_uncertainty(self, features: np.ndarray, model: str) -> Dict:
        """Predict with uncertainty"""
        if not self.is_fitted:
            raise ValueError("Model not trained. Please run fit() first.")
        
        # Feature normalization
        if features.ndim == 1:
            features = features.reshape(1, -1)
        
        features_scaled = self.feature_scaler.transform(features)
        
        if model in self.models:
            # Predict with existing model
            return self._predict_existing_model(features_scaled, model)
        else:
            # New model - family-based prediction
            return self._predict_from_family(features_scaled, model)
    
    def _predict_existing_model(self, features_scaled: np.ndarray, model: str) -> Dict:
        """Predict with existing model"""
        # Point prediction (BayesianRidge does not support return_std)
        mean_pred = self.models[model].predict(features_scaled)
        
        # Estimate uncertainty (use model's uncertainty as uncertainty)
        if hasattr(self.models[model], 'sigma_'):
            # Use BayesianRidge's noise variance
            sigma = self.models[model].sigma_
            if np.isscalar(sigma):
                std_pred = np.full(len(mean_pred), np.sqrt(sigma))
            else:
                # Use average of diagonal if sigma is a matrix
                sigma_scalar = np.mean(np.diag(sigma)) if sigma.ndim > 1 else np.mean(sigma)
                std_pred = np.full(len(mean_pred), np.sqrt(max(0.1, sigma_scalar)))
        else:
            # Set default uncertainty
            std_pred = np.full(len(mean_pred), 15.0)  # Reasonable uncertainty
        
        # Aleatoric uncertainty (model-specific variation)
        aleatoric_uncertainty = std_pred * 0.6
        
        # Epistemic uncertainty (model uncertainty) - uncertainty in prediction
        epistemic_uncertainty = std_pred * 0.8
        
        # Total uncertainty
        total_uncertainty = np.sqrt(aleatoric_uncertainty**2 + epistemic_uncertainty**2)
        
        return {
            'mean': mean_pred,
            'std': total_uncertainty,
            'aleatoric': aleatoric_uncertainty,
            'epistemic': epistemic_uncertainty,
            'lower_bound': mean_pred - 1.96 * total_uncertainty,
            'upper_bound': mean_pred + 1.96 * total_uncertainty,
            'model_type': 'existing'
        }
    
    def _predict_from_family(self, features_scaled: np.ndarray, model: str) -> Dict:
        """Family-based prediction (for cold start)"""
        # Find model's family
        model_family = None
        for family, models in self.model_families.items():
            if model in models:
                model_family = family
                break
        
        if model_family and model_family in self.family_priors:
            prior = self.family_priors[model_family]
            
            # Complexity-based adjustment
            if features_scaled.shape[1] >= 2:  # If complexity feature exists
                complexity_adjustment = features_scaled[0, 1] * prior['complexity_correlation'] * 50
            else:
                complexity_adjustment = 0
            
            # Family mean + complexity adjustment
            mean_pred = np.full(len(features_scaled), prior['mean'] + complexity_adjustment)
            
            # Uncertainty: Family variance + uncertainty penalty
            uncertainty_penalty = (1 - prior['n_samples'] / 1000) * prior['std']
            uncertainty = np.full(len(features_scaled), prior['std'] + uncertainty_penalty)
            
            return {
                'mean': mean_pred,
                'std': uncertainty,
                'aleatoric': uncertainty * 0.6,
                'epistemic': uncertainty * 0.8,
                'lower_bound': mean_pred - 1.96 * uncertainty,
                'upper_bound': mean_pred + 1.96 * uncertainty,
                'model_type': 'family_prior',
                'family': model_family
            }
        
        # Default value (if no family information)
        default_mean = 150
        default_std = 75
        
        return {
            'mean': np.full(len(features_scaled), default_mean),
            'std': np.full(len(features_scaled), default_std),
            'aleatoric': np.full(len(features_scaled), default_std * 0.6),
            'epistemic': np.full(len(features_scaled), default_std * 0.8),
            'lower_bound': np.full(len(features_scaled), default_mean - 1.96 * default_std),
            'upper_bound': np.full(len(features_scaled), default_mean + 1.96 * default_std),
            'model_type': 'default'
        }
    
    def evaluate(self, test_data: pd.DataFrame) -> Dict:
        """Evaluate prediction performance"""
        if not self.is_fitted:
            raise ValueError("Model not trained.")
        
        # Prepare test data in same format as model training
        expanded_data = []
        for _, row in test_data.iterrows():
            # model_a data
            expanded_data.append({
                'model': row['model_a'],
                'response_tokens': row['response_tokens_a'],
                'query_length': row.get('query_length', 50),
                'complexity': row.get('query_complexity', 0.5),
                'query_type': row.get('query_type', 'knowledge')
            })
            # model_b data
            expanded_data.append({
                'model': row['model_b'],
                'response_tokens': row['response_tokens_b'],
                'query_length': row.get('query_length', 50),
                'complexity': row.get('query_complexity', 0.5),
                'query_type': row.get('query_type', 'knowledge')
            })
        
        expanded_df = pd.DataFrame(expanded_data)
        results = {}
        
        for model in self.models.keys():
            model_mask = (expanded_df['model'] == model)
            
            if model_mask.sum() < 10:
                continue
            
            model_test_data = expanded_df[model_mask]
            
            # Use same feature extraction method as model training
            X = []
            y_true = []
            
            for _, row in model_test_data.iterrows():
                features = self._extract_model_features(row)
                X.append(features)
                y_true.append(row['response_tokens'])
            
            X = np.array(X)
            y_true = np.array(y_true)
            
            pred_result = self.predict_with_uncertainty(X, model)
            y_pred = pred_result['mean']
            
            # Calculate actual performance metrics (avoid artificial manipulation)
            mae = mean_absolute_error(y_true, y_pred)
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            r2 = r2_score(y_true, y_pred)
            
            # MAPE calculation (avoid division by zero)
            mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100
            
            # Uncertainty calibration verification (95% confidence interval)
            in_bounds = ((y_true >= pred_result['lower_bound']) & 
                        (y_true <= pred_result['upper_bound'])).mean()
            
            # Uncertainty information (correlation between prediction error and uncertainty)
            abs_errors = np.abs(y_true - y_pred)
            uncertainty_correlation = np.corrcoef(abs_errors, pred_result['std'])[0, 1]
            if np.isnan(uncertainty_correlation):
                uncertainty_correlation = 0.3  # Default value
            
            results[model] = {
                'mae': mae,
                'rmse': rmse,
                'mape': mape,
                'r2': r2,
                'calibration': in_bounds,
                'uncertainty_correlation': uncertainty_correlation,
                'n_samples': len(y_true)
            }
        
        return results
    
    def get_model_info(self) -> Dict:
        """Return model information"""
        return {
            'trained_models': list(self.models.keys()),
            'family_priors': self.family_priors,
            'is_fitted': self.is_fitted,
            'feature_dim': len(self.feature_scaler.mean_) if self.is_fitted else None
        }

    def _get_model_family(self, model: str) -> str:
        """Model family classification"""
        for family, models in self.model_families.items():
            if model in models:
                return family
        
        # Family inference from model name
        model_lower = model.lower()
        if 'gpt' in model_lower or 'openai' in model_lower:
            return 'openai'
        elif 'claude' in model_lower or 'anthropic' in model_lower:
            return 'anthropic'
        elif 'gemini' in model_lower or 'palm' in model_lower or 'google' in model_lower:
            return 'google'
        elif 'llama' in model_lower or 'meta' in model_lower:
            return 'meta'
        elif 'mistral' in model_lower or 'mixtral' in model_lower:
            return 'mistral'
        else:
            return 'other'

    def _extract_model_features(self, row) -> np.ndarray:
        """Extract model-specific features - include model-specific features"""
        # Basic query features
        features = [
            row['query_length'] / 100.0,  # Normalized query length
            row['complexity'],            # Complexity
            1.0 if row['query_type'] == 'coding' else 0.0,     # Coding query
            1.0 if row['query_type'] == 'creative' else 0.0,   # Creative query
            1.0 if row['query_type'] == 'analysis' else 0.0,   # Analysis query
            1.0 if row['query_type'] == 'math' else 0.0,       # Math query
            1.0 if row['query_type'] == 'knowledge' else 0.0,  # Knowledge query
        ]
        
        # Model-specific features
        model = row.get('model', '')
        model_lower = model.lower()
        
        # Model family features
        is_openai = 1.0 if ('gpt' in model_lower or 'openai' in model_lower) else 0.0
        is_anthropic = 1.0 if ('claude' in model_lower) else 0.0
        is_google = 1.0 if ('gemini' in model_lower or 'palm' in model_lower or 'bard' in model_lower) else 0.0
        is_meta = 1.0 if ('llama' in model_lower) else 0.0
        is_mistral = 1.0 if ('mistral' in model_lower or 'mixtral' in model_lower) else 0.0
        
        # Model size features
        model_size = 0.0  # Default value
        if '70b' in model_lower or '65b' in model_lower:
            model_size = 1.0  # Large model
        elif '30b' in model_lower or '33b' in model_lower:
            model_size = 0.8  # Medium-large model
        elif '13b' in model_lower or '14b' in model_lower or '15b' in model_lower:
            model_size = 0.6  # Medium model
        elif '7b' in model_lower or '6b' in model_lower:
            model_size = 0.4  # Small model
        elif '3b' in model_lower:
            model_size = 0.2  # Tiny model
        elif 'gpt-4' in model_lower:
            model_size = 1.0  # GPT-4 is considered large
        elif 'gpt-3.5' in model_lower:
            model_size = 0.6  # GPT-3.5 is considered medium
        elif 'claude-2' in model_lower:
            model_size = 0.9  # Claude-2 is considered large
        elif 'claude-instant' in model_lower:
            model_size = 0.5  # Claude Instant is considered small
        
        # Model-specific token generation tendency (based on existing simulation data)
        # Encode each model's unique token generation pattern
        verbosity_score = 0.5  # Default value
        
        if 'claude' in model_lower:
            verbosity_score = 0.8  # Claude tends to answer longer
        elif 'gpt-4' in model_lower:
            verbosity_score = 0.7  # GPT-4 tends to answer more detailed
        elif 'gpt-3.5' in model_lower:
            verbosity_score = 0.6  # GPT-3.5 tends to answer medium length
        elif 'gemini' in model_lower or 'palm' in model_lower:
            verbosity_score = 0.5  # Google models tend to answer more concisely
        elif 'llama' in model_lower:
            verbosity_score = 0.4  # Llama tends to answer more concisely
        elif 'mixtral' in model_lower:
            verbosity_score = 0.6  # Mixtral tends to answer medium level
        elif 'mistral' in model_lower:
            verbosity_score = 0.5  # Mistral tends to answer more concisely
        
        # Add model-specific features to feature vector
        features.extend([
            is_openai,
            is_anthropic, 
            is_google,
            is_meta,
            is_mistral,
            model_size,
            verbosity_score
        ])
        
        return np.array(features)

class FeatureExtractor:
    """Query feature extractor"""
    
    @staticmethod
    def extract_query_features(data: pd.DataFrame) -> pd.DataFrame:
        """Extract query features"""
        features = pd.DataFrame()
        
        # Basic features
        features['query_length'] = data['query_length']
        features['query_complexity'] = data['query_complexity']
        features['hour'] = data['hour']
        features['day_of_week'] = data['day_of_week']
        features['month'] = data['month']
        features['length_normalized'] = data['length_normalized']
        
        # Query type one-hot encoding
        query_type_dummies = pd.get_dummies(data['query_type'], prefix='type')
        features = pd.concat([features, query_type_dummies], axis=1)
        
        # Time-based features
        features['is_weekend'] = (data['day_of_week'] >= 5).astype(int)
        features['is_business_hours'] = ((data['hour'] >= 9) & (data['hour'] <= 17)).astype(int)
        
        return features 