import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class ThompsonSamplingRouter:
    """Thompson Sampling ê¸°ë°˜ LLM ë¼ìš°í„°"""
    
    def __init__(self, models: List[str], token_predictor, cost_weight: float = 0.01):
        self.models = models
        self.token_predictor = token_predictor
        self.cost_weight = cost_weight
        
        # ë² íƒ€ ë¶„í¬ íŒŒë¼ë¯¸í„° (ì„±ê³µ, ì‹¤íŒ¨ ì¹´ìš´íŠ¸)
        self.alpha = {model: 1.0 for model in models}
        self.beta = {model: 1.0 for model in models}
        
        # ëª¨ë¸ë³„ ë¹„ìš© ì •ë³´
        self.model_costs = {
            'gpt-4-1106-preview': 0.03,
            'gpt-4-0613': 0.03,
            'claude-2.1': 0.024,
            'claude-2.0': 0.024,
            'claude-instant-1': 0.00163,
            'gemini-pro': 0.0005,
            'llama-2-70b-chat': 0.00275,
            'llama-2-13b-chat': 0.0013,
            'mixtral-8x7b-instruct': 0.00024,
            'gpt-3.5-turbo-0613': 0.002
        }
        
        # ë¼ìš°íŒ… íˆìŠ¤í† ë¦¬
        self.routing_history = []
        self.performance_history = []
        self.exploration_rate = 0.1  # íƒí—˜ ë¹„ìœ¨
        
    def set_informative_prior(self, model: str, family_performance: Optional[float] = None, 
                             confidence: float = 0.5):
        """ì •ë³´ì  ì‚¬ì „ ë¶„í¬ ì„¤ì • (ì½œë“œ ìŠ¤íƒ€íŠ¸ìš©)"""
        
        if family_performance is not None:
            # ë² íƒ€ ë¶„í¬ íŒŒë¼ë¯¸í„° ë³€í™˜ (Method of Moments)
            mean_perf = max(0.01, min(0.99, family_performance))
            variance = confidence * (1 - confidence)
            
            if 0 < mean_perf < 1 and variance > 0:
                # ë² íƒ€ ë¶„í¬ì˜ í‰ê· ê³¼ ë¶„ì‚°ìœ¼ë¡œë¶€í„° Î±, Î² ê³„ì‚°
                common_factor = mean_perf * (1 - mean_perf) / variance - 1
                alpha = mean_perf * common_factor
                beta = (1 - mean_perf) * common_factor
                
                self.alpha[model] = max(1.0, alpha)
                self.beta[model] = max(1.0, beta)
                
                print(f"ğŸ¯ {model} ì‚¬ì „ ë¶„í¬ ì„¤ì •: Î±={self.alpha[model]:.2f}, Î²={self.beta[model]:.2f}")
    
    def select_model(self, query_features: np.ndarray, model_predictions: Dict[str, float], 
                    model_uncertainties: Dict[str, float], available_models: Optional[List[str]] = None, 
                    risk_tolerance: float = 0.25) -> str:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ìµœì  ëª¨ë¸ ì„ íƒ (ë² ì´ì§€ì•ˆ ì˜ˆì¸¡ ê²°ê³¼ í™œìš©)"""
        
        if available_models is None:
            available_models = self.models
            
        utilities = {}
        predictions = {}
        
        # íƒí—˜ vs í™œìš© ê²°ì •
        should_explore = np.random.random() < self.exploration_rate
        
        for model in available_models:
            if model not in model_predictions:
                continue
                
            # ì„±ëŠ¥ ìƒ˜í”Œë§ (ë² íƒ€ ë¶„í¬ì—ì„œ)
            performance_sample = np.random.beta(self.alpha[model], self.beta[model])
            
            # ë² ì´ì§€ì•ˆ ì˜ˆì¸¡ ê²°ê³¼ ì‚¬ìš©
            expected_tokens = model_predictions[model]
            uncertainty = model_uncertainties[model]
            
            # ì˜ˆìƒ ë¹„ìš© ê³„ì‚°
            cost_per_1k = self.model_costs.get(model, 0.01)
            expected_cost = (expected_tokens / 1000) * cost_per_1k
            
            # ë¶ˆí™•ì‹¤ì„± í˜ë„í‹° ì ìš©
            uncertainty_penalty = risk_tolerance * uncertainty / 100
            
            if should_explore:
                # íƒí—˜: ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ ëª¨ë¸ ì„ í˜¸
                exploration_bonus = uncertainty / 100
                utility = performance_sample + exploration_bonus - self.cost_weight * expected_cost
            else:
                # í™œìš©: ì„±ëŠ¥ ìš°ì„ , ë¶ˆí™•ì‹¤ì„± í˜ë„í‹°
                utility = performance_sample - self.cost_weight * expected_cost - uncertainty_penalty
            
            utilities[model] = utility
            predictions[model] = {
                'performance_sample': performance_sample,
                'expected_cost': expected_cost,
                'uncertainty': uncertainty,
                'tokens': expected_tokens,
                'exploration_mode': should_explore
            }
        
        # ìµœê³  íš¨ìš©ì˜ ëª¨ë¸ ì„ íƒ
        selected_model = max(utilities, key=utilities.get)
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.routing_history.append({
            'selected_model': selected_model,
            'utilities': utilities.copy(),
            'predictions': predictions.copy(),
            'timestamp': pd.Timestamp.now(),
            'exploration_mode': should_explore
        })
        
        return selected_model
    
    def update(self, model: str, actual_performance: float, actual_cost: Optional[float] = None):
        """ê´€ì°°ëœ ê²°ê³¼ë¡œ ë² íƒ€ ë¶„í¬ ì—…ë°ì´íŠ¸"""
        
        # ì„±ê³¼ ê¸°ì¤€: ì„ê³„ê°’ ì´ìƒì´ë©´ ì„±ê³µ
        performance_threshold = 0.5
        
        if actual_performance > performance_threshold:
            self.alpha[model] += 1
        else:
            self.beta[model] += 1
        
        # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì €ì¥
        self.performance_history.append({
            'model': model,
            'performance': actual_performance,
            'cost': actual_cost,
            'timestamp': pd.Timestamp.now()
        })
        
        # ì ì‘ì  íƒí—˜ë¥  ì¡°ì • (ì„±ëŠ¥ì´ ì•ˆì •ë˜ë©´ íƒí—˜ ê°ì†Œ)
        if len(self.performance_history) > 100:
            recent_variance = np.var([h['performance'] for h in self.performance_history[-50:]])
            if recent_variance < 0.01:  # ì„±ëŠ¥ì´ ì•ˆì •ë¨
                self.exploration_rate = max(0.05, self.exploration_rate * 0.95)
    
    def get_model_confidence(self, model: str) -> Dict:
        """ëª¨ë¸ë³„ ì‹ ë¢°ë„ ê³„ì‚°"""
        alpha = self.alpha[model]
        beta = self.beta[model]
        
        mean = alpha / (alpha + beta)
        variance = (alpha * beta) / ((alpha + beta)**2 * (alpha + beta + 1))
        std = np.sqrt(variance)
        
        # 95% ì‹ ë¢°êµ¬ê°„ (ë² íƒ€ ë¶„í¬)
        ci_lower = stats.beta.ppf(0.025, alpha, beta)
        ci_upper = stats.beta.ppf(0.975, alpha, beta)
        
        return {
            'mean_performance': mean,
            'variance': variance,
            'std': std,
            'confidence_interval': [ci_lower, ci_upper],
            'samples': alpha + beta - 2,  # ì´ˆê¸°ê°’ 2 ì œì™¸
            'alpha': alpha,
            'beta': beta
        }
    
    def get_routing_stats(self) -> Dict:
        """ë¼ìš°íŒ… í†µê³„ ë°˜í™˜"""
        if not self.routing_history:
            return {}
        
        # ëª¨ë¸ ì„ íƒ ë¶„í¬
        model_counts = {}
        exploration_counts = 0
        
        for entry in self.routing_history:
            model = entry['selected_model']
            model_counts[model] = model_counts.get(model, 0) + 1
            if entry.get('exploration_mode', False):
                exploration_counts += 1
        
        total_decisions = len(self.routing_history)
        
        # ì„±ëŠ¥ í†µê³„
        performance_stats = {}
        if self.performance_history:
            recent_performances = [h['performance'] for h in self.performance_history[-100:]]
            performance_stats = {
                'mean_performance': np.mean(recent_performances),
                'std_performance': np.std(recent_performances),
                'recent_trend': self._calculate_trend(recent_performances)
            }
        
        return {
            'total_decisions': total_decisions,
            'model_distribution': {
                model: count/total_decisions 
                for model, count in model_counts.items()
            },
            'exploration_rate': exploration_counts / total_decisions if total_decisions > 0 else 0,
            'confidence_stats': {
                model: self.get_model_confidence(model) 
                for model in self.models
            },
            'performance_stats': performance_stats,
            'current_exploration_rate': self.exploration_rate
        }
    
    def _calculate_trend(self, values: List[float], window: int = 20) -> str:
        """ìµœê·¼ ì„±ëŠ¥ íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(values) < window:
            return "insufficient_data"
        
        recent = values[-window:]
        earlier = values[-2*window:-window] if len(values) >= 2*window else values[:-window]
        
        if np.mean(recent) > np.mean(earlier) + 0.05:
            return "improving"
        elif np.mean(recent) < np.mean(earlier) - 0.05:
            return "declining"
        else:
            return "stable"

    def update_rewards(self, model: str, actual_performance: float, query_data: Optional[pd.Series] = None):
        """ê´€ì°°ëœ ê²°ê³¼ë¡œ ë² íƒ€ ë¶„í¬ ì—…ë°ì´íŠ¸ (main_experiment.py í˜¸í™˜)"""
        self.update(model, actual_performance)
    
    def get_selection_stats(self) -> Dict[str, int]:
        """ëª¨ë¸ë³„ ì„ íƒ í†µê³„ ë°˜í™˜"""
        if not self.routing_history:
            return {model: 0 for model in self.models}
        
        model_counts = {}
        for entry in self.routing_history:
            model = entry['selected_model']
            model_counts[model] = model_counts.get(model, 0) + 1
        
        # ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        stats = {model: 0 for model in self.models}
        stats.update(model_counts)
        
        return stats
    
    def get_model_preferences(self) -> Dict[str, float]:
        """ëª¨ë¸ë³„ ì„ í˜¸ë„ (í‰ê·  ì„±ëŠ¥) ë°˜í™˜"""
        preferences = {}
        for model in self.models:
            confidence = self.get_model_confidence(model)
            preferences[model] = confidence['mean_performance']
        
        return preferences
    
    def get_exploration_rate(self) -> float:
        """í˜„ì¬ íƒí—˜ë¥  ë°˜í™˜"""
        return self.exploration_rate
    
    def check_convergence(self, window: int = 50, threshold: float = 0.05) -> bool:
        """ìˆ˜ë ´ ì—¬ë¶€ í™•ì¸"""
        if len(self.performance_history) < window:
            return False
        
        recent_performances = [h['performance'] for h in self.performance_history[-window:]]
        performance_variance = np.var(recent_performances)
        
        return performance_variance < threshold

    def add_new_model(self, model: str, alpha: float = 1.0, beta: float = 1.0):
        """ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€"""
        if model not in self.models:
            self.models.append(model)
        
        self.alpha[model] = alpha
        self.beta[model] = beta
        
        print(f"â• ìƒˆ ëª¨ë¸ ì¶”ê°€: {model} (Î±={alpha}, Î²={beta})")

class ColdStartExperiment:
    """ì½œë“œ ìŠ¤íƒ€íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í—˜"""
    
    def __init__(self, predictor, router_class):
        self.predictor = predictor
        self.router_class = router_class
        self.results = {}
    
    def simulate_cold_start(self, new_model: str, family: str, test_data: pd.DataFrame, 
                          n_iterations: int = 500) -> Dict:
        """ì½œë“œ ìŠ¤íƒ€íŠ¸ ì‹œë®¬ë ˆì´ì…˜"""
        
        print(f"\nâ„ï¸ ì½œë“œ ìŠ¤íƒ€íŠ¸ ì‹œë®¬ë ˆì´ì…˜: {new_model}")
        
        # ìƒˆ ëª¨ë¸ ì œì™¸í•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        available_models = [m for m in self.predictor.get_model_info()['trained_models'] 
                           if m != new_model]
        available_models.append(new_model)  # ìƒˆ ëª¨ë¸ ì¶”ê°€
        
        # ë¼ìš°í„° ì´ˆê¸°í™”
        router = self.router_class(available_models, self.predictor)
        
        # ê°€ì¡± ê¸°ë°˜ ì‚¬ì „ ë¶„í¬ ì„¤ì •
        family_priors = self.predictor.family_priors
        if family in family_priors:
            family_performance = (family_priors[family]['mean'] - 100) / 200  # 0-1 ì •ê·œí™”
            router.set_informative_prior(
                new_model, 
                family_performance, 
                confidence=0.3
            )
        
        # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ì €ì¥
        results = {
            'iterations': [],
            'selected_models': [],
            'regrets': [],
            'cumulative_regret': [],
            'convergence_metrics': [],
            'new_model_selections': 0,
            'performance_history': []
        }
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìƒ˜í”Œë§
        test_sample = test_data.sample(min(n_iterations, len(test_data)), random_state=42)
        
        cumulative_regret = 0
        convergence_window = []
        
        for i, (idx, row) in enumerate(test_sample.iterrows()):
            # ì¿¼ë¦¬ íŠ¹ì„± ì¶”ì¶œ
            query_features = self.predictor._extract_query_features(row)
            
            # ëª¨ë¸ ì„ íƒ
            try:
                # í† í° ì˜ˆì¸¡ ìˆ˜í–‰
                predicted_tokens = {}
                model_uncertainties = {}
                
                for model in available_models:
                    if model == new_model:
                        # ì‹ ê·œ ëª¨ë¸: ê°€ì¡± í‰ê·  ì‚¬ìš©
                        if family in family_priors:
                            pred_result = {
                                'mean': [family_priors[family]['mean']],
                                'std': [family_priors[family]['std']]
                            }
                        else:
                            pred_result = {'mean': [150.0], 'std': [50.0]}
                    else:
                        pred_result = self.predictor.predict_with_uncertainty(query_features, model)
                    
                    predicted_tokens[model] = pred_result['mean'][0]
                    model_uncertainties[model] = pred_result['std'][0]
                
                selected_model = router.select_model(
                    query_features, predicted_tokens, model_uncertainties, available_models, risk_tolerance=0.25
                )
                utility = 0  # placeholder
                prediction = {}  # placeholder
            except Exception as e:
                print(f"âš ï¸ ë¼ìš°íŒ… ì˜¤ë¥˜: {e}")
                selected_model = np.random.choice(available_models)
                utility = 0
                prediction = {}
            
            # ì‹¤ì œ ìµœì  ëª¨ë¸ ê²°ì • (ground truth)
            true_optimal = self._get_optimal_model(row, available_models)
            
            # í›„íšŒ ê³„ì‚°
            regret = 1.0 if selected_model != true_optimal else 0.0
            cumulative_regret += regret
            
            # ì‹¤ì œ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜
            actual_performance = self._simulate_actual_performance(selected_model, row)
            
            # ë¼ìš°í„° ì—…ë°ì´íŠ¸
            router.update(selected_model, actual_performance)
            
            # ê²°ê³¼ ì €ì¥
            results['iterations'].append(i + 1)
            results['selected_models'].append(selected_model)
            results['regrets'].append(regret)
            results['cumulative_regret'].append(cumulative_regret)
            results['performance_history'].append(actual_performance)
            
            if selected_model == new_model:
                results['new_model_selections'] += 1
            
            # ìˆ˜ë ´ ë©”íŠ¸ë¦­ (50 ì¿¼ë¦¬ ìœˆë„ìš°)
            convergence_window.append(regret)
            if len(convergence_window) > 50:
                convergence_window.pop(0)
            
            if len(convergence_window) == 50:
                convergence_score = 1.0 - np.mean(convergence_window)
                results['convergence_metrics'].append(convergence_score)
        
        # ìµœì¢… í†µê³„
        final_stats = {
            'total_iterations': len(results['iterations']),
            'final_cumulative_regret': results['cumulative_regret'][-1],
            'average_regret': results['cumulative_regret'][-1] / len(results['iterations']),
            'new_model_usage_rate': results['new_model_selections'] / len(results['iterations']),
            'convergence_time': self._find_convergence_time(results['regrets']),
            'final_convergence_score': results['convergence_metrics'][-1] if results['convergence_metrics'] else 0,
            'router_stats': router.get_routing_stats(),
            'performance_improvement': self._calculate_performance_improvement(results['performance_history'])
        }
        
        results['final_stats'] = final_stats
        return results
    
    def _get_optimal_model(self, row: pd.Series, available_models: List[str]) -> str:
        """ì‹¤ì œ ìµœì  ëª¨ë¸ ê²°ì • (í’ˆì§ˆê³¼ ë¹„ìš© ê³ ë ¤)"""
        model_scores = {}
        
        # í’ˆì§ˆ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš° í™œìš©
        if hasattr(row, 'quality_a') and hasattr(row, 'quality_b'):
            model_qualities = {
                row['model_a']: getattr(row, 'quality_a', 1000),
                row['model_b']: getattr(row, 'quality_b', 1000)
            }
        else:
            # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜
            model_qualities = {
                'gpt-4-1106-preview': 1250,
                'gpt-4-0613': 1200,
                'claude-2.1': 1180,
                'claude-2.0': 1150,
                'gemini-pro': 1140,
                'mixtral-8x7b-instruct': 1120,
                'llama-2-70b-chat': 1080,
                'gpt-3.5-turbo-0613': 1050,
                'llama-2-13b-chat': 1000,
                'claude-instant-1': 970
            }
        
        for model in available_models:
            if model in model_qualities:
                base_score = model_qualities[model]
                
                # ë³µì¡ë„ ë³´ë„ˆìŠ¤
                complexity_bonus = getattr(row, 'query_complexity', 0.5) * 50
                
                # ë¹„ìš© í˜ë„í‹°
                cost_penalty = self.predictor.model_costs.get(model, 0.01) * 1000
                
                final_score = base_score + complexity_bonus - cost_penalty
                model_scores[model] = final_score
        
        return max(model_scores, key=model_scores.get) if model_scores else available_models[0]
    
    def _simulate_actual_performance(self, model: str, row: pd.Series) -> float:
        """ì‹¤ì œ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜"""
        # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì„±ëŠ¥ ê³„ì‚°
        if hasattr(row, 'quality_a') and row['model_a'] == model:
            base_performance = (getattr(row, 'quality_a', 1000) - 950) / 300
        elif hasattr(row, 'quality_b') and row['model_b'] == model:
            base_performance = (getattr(row, 'quality_b', 1000) - 950) / 300
        else:
            # ê¸°ë³¸ ì„±ëŠ¥ ë§¤í•‘
            performance_map = {
                'gpt-4-1106-preview': 0.85,
                'gpt-4-0613': 0.80,
                'claude-2.1': 0.78,
                'claude-2.0': 0.75,
                'gemini-pro': 0.73,
                'mixtral-8x7b-instruct': 0.70,
                'llama-2-70b-chat': 0.65,
                'gpt-3.5-turbo-0613': 0.60,
                'llama-2-13b-chat': 0.55,
                'claude-instant-1': 0.50
            }
            base_performance = performance_map.get(model, 0.5)
        
        # ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = np.random.normal(0, 0.1)
        return max(0.1, min(0.9, base_performance + noise))
    
    def _find_convergence_time(self, regrets: List[float], threshold: float = 0.2, 
                              window: int = 50) -> int:
        """ìˆ˜ë ´ ì‹œê°„ ì°¾ê¸°"""
        if len(regrets) < window:
            return len(regrets)
        
        for i in range(window, len(regrets)):
            window_regrets = regrets[i-window:i]
            if np.mean(window_regrets) <= threshold:
                return i - window
        
        return len(regrets)
    
    def _calculate_performance_improvement(self, performances: List[float], 
                                         window: int = 50) -> float:
        """ì„±ëŠ¥ ê°œì„ ìœ¨ ê³„ì‚°"""
        if len(performances) < window:
            return 0.0
        
        initial_perf = np.mean(performances[:window])
        final_perf = np.mean(performances[-window:])
        
        return (final_perf - initial_perf) / initial_perf if initial_perf > 0 else 0.0 